{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173c94b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c19519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      on 13th and or 14th day of January 1958, they ...\n",
      "1      According to prosecution, accused 1 and 2 comm...\n",
      "2      The briefly stated prosecution story is that o...\n",
      "3      Sole accused in this case was sent up for tria...\n",
      "4      The case of prosecution as unfolded by report ...\n",
      "                             ...                        \n",
      "172    instant case pertains to the abduction of PWs ...\n",
      "173    Victim, aged about six years, was allegedly ki...\n",
      "174    Planned kidnapping of 3 minor children aged ab...\n",
      "175               Kidnapping in order to compel marriage\n",
      "176       Kidnapping of two children aged 6 and 8 years \n",
      "Name: sentence, Length: 177, dtype: object\n",
      "(array([302, 364, 375, 383, 390]), array([50, 22, 31, 24, 50]))\n"
     ]
    }
   ],
   "source": [
    "# Read the input dataset \n",
    "filepath_dire = {'data': './ACPD.csv'}\n",
    "data_list = []\n",
    "for source, filepath in filepath_dire.items():\n",
    " data = pd.read_csv(filepath, names=['sentence', 'label'], sep=',')\n",
    " data['source'] = source  # Add another column filled with the source name\n",
    " data_list.append(data)\n",
    "data = pd.concat(data_list)\n",
    "x= data['sentence']\n",
    "y= data['label']\n",
    "print(x)\n",
    "\n",
    "#there are {5} unique classes for classification\n",
    "print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "839977a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   4   2 155]\n",
      " [  0   0   0 ...  37  33  91]\n",
      " [  0   0   0 ...   1   3 281]\n",
      " ...\n",
      " [  0   0   0 ... 138  22 127]\n",
      " [  0   0   0 ...   5   4 262]\n",
      " [  0   0   0 ...   2 109  62]]\n"
     ]
    }
   ],
   "source": [
    "tk = Tokenizer(num_words= 400, filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True, split=\" \")\n",
    "tk.fit_on_texts(x)\n",
    "x = tk.texts_to_sequences(x)\n",
    "x = sequence.pad_sequences(x, maxlen=150)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b262eb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      390\n",
      "1      390\n",
      "2      390\n",
      "3      390\n",
      "4      390\n",
      "      ... \n",
      "172    364\n",
      "173    364\n",
      "174    364\n",
      "175    364\n",
      "176    364\n",
      "Name: label, Length: 177, dtype: int64\n",
      "(array([0, 1, 2, 3, 4]), array([50, 22, 31, 24, 50]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_Y = LabelEncoder()\n",
    "print(y)\n",
    "y = labelencoder_Y.fit_transform(y)\n",
    "print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6fb677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras import utils as np_utils\n",
    "print(y)\n",
    "y = np_utils.to_categorical(y, num_classes= 5)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f9911b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   4   2 155]\n",
      " [  0   0   0 ...  37  33  91]\n",
      " [  0   0   0 ...   1   3 281]\n",
      " ...\n",
      " [  0   0   0 ... 138  22 127]\n",
      " [  0   0   0 ...   5   4 262]\n",
      " [  0   0   0 ...   2 109  62]]\n",
      "[[  0   0   0 ...   5   3  86]\n",
      " [  0   0   0 ...   4   1  31]\n",
      " [  0   0   0 ...   5  12  25]\n",
      " ...\n",
      " [  0   0   0 ...   2 156   2]\n",
      " [  0   0   0 ...   8  92  42]\n",
      " [  0   0   0 ...  90 105  41]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(150)\n",
    "indices = np.arange(len(x))\n",
    "np.random.shuffle(indices)\n",
    "print(x)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b937a6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 12, 31, 4, 78, 6, 15, 92, 4, 21, 16, 4, 8, 6, 89]\n"
     ]
    }
   ],
   "source": [
    "index_from=3\n",
    "start_char = 1\n",
    "if start_char is not None:\n",
    "        x = [[start_char] + [w + index_from for w in x1] for x1 in x]\n",
    "elif index_from:\n",
    "        x = [[w + index_from for w in x1] for x1 in x]\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36e93b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 151)\n",
      "(141, 5)\n",
      "(36, 151)\n",
      "(36, 5)\n"
     ]
    }
   ],
   "source": [
    "num_words = None\n",
    "if not num_words:\n",
    "        num_words = max([max(x1) for x1 in x])\n",
    "        \n",
    "oov_char = 2\n",
    "skip_top = 0\n",
    "# by convention, use 2 as OOV word\n",
    "# reserve 'index_from' (=3 by default) characters:\n",
    "# 0 (padding), 1 (start), 2 (OOV)\n",
    "if oov_char is not None:\n",
    "        x = [[w if (skip_top <= w < num_words) else oov_char for w in x1] for x1 in x]\n",
    "else:\n",
    "        x = [[w for w in x1 if (skip_top <= w < num_words)] for x1 in x]\n",
    "# split test and train data\n",
    "test_split = 0.2\n",
    "idx = int(len(x) * (1 - test_split))\n",
    "x_train, y_train = np.array(x[:idx]), np.array(y[:idx])\n",
    "x_test, y_test = np.array(x[idx:]), np.array(y[idx:])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90e26ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (141, 151)\n",
      "x_test shape: (36, 151)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=151)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=151)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0101ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-23 23:31:16.021837: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-23 23:31:16.024585: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 4. Tune using inter_op_parallelism_threads for best performance.\n",
      "/Users/himanshu/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 141 samples, validate on 36 samples\n",
      "Epoch 1/12\n",
      "141/141 [==============================] - 3s 21ms/step - loss: 1.5556 - accuracy: 0.2979 - val_loss: 1.6058 - val_accuracy: 0.2500\n",
      "Epoch 2/12\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 1.4993 - accuracy: 0.3333 - val_loss: 1.6174 - val_accuracy: 0.2500\n",
      "Epoch 3/12\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 1.4353 - accuracy: 0.3830 - val_loss: 1.6073 - val_accuracy: 0.3056\n",
      "Epoch 4/12\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 1.3645 - accuracy: 0.4823 - val_loss: 1.6295 - val_accuracy: 0.2500\n",
      "Epoch 5/12\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 1.2163 - accuracy: 0.5957 - val_loss: 1.4044 - val_accuracy: 0.4167\n",
      "Epoch 6/12\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 1.0358 - accuracy: 0.6028 - val_loss: 1.2573 - val_accuracy: 0.4722\n",
      "Epoch 7/12\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 0.8798 - accuracy: 0.6596 - val_loss: 1.1800 - val_accuracy: 0.5278\n",
      "Epoch 8/12\n",
      "141/141 [==============================] - 1s 7ms/step - loss: 0.7369 - accuracy: 0.7660 - val_loss: 1.1390 - val_accuracy: 0.6389\n",
      "Epoch 9/12\n",
      "141/141 [==============================] - 1s 7ms/step - loss: 0.6321 - accuracy: 0.8298 - val_loss: 1.1290 - val_accuracy: 0.6111\n",
      "Epoch 10/12\n",
      "141/141 [==============================] - 1s 7ms/step - loss: 0.5170 - accuracy: 0.8794 - val_loss: 1.0368 - val_accuracy: 0.7500\n",
      "Epoch 11/12\n",
      "141/141 [==============================] - 1s 7ms/step - loss: 0.4122 - accuracy: 0.9078 - val_loss: 1.0583 - val_accuracy: 0.7222\n",
      "Epoch 12/12\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 0.3241 - accuracy: 0.9433 - val_loss: 1.0550 - val_accuracy: 0.6389\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 151, 50)           50000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 151, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 149, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1255      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 151,755\n",
      "Trainable params: 151,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = 1000\n",
    "maxlen = 151\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "\n",
    "# CNN with max pooling imeplementation \n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=10,\n",
    "          epochs=12,\n",
    "          validation_data=(x_test, y_test))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af78820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3  3 ...  8  6 89]\n",
      " [ 1  3  3 ...  7  4 34]\n",
      " [ 1  3  3 ...  8 15 28]\n",
      " ...\n",
      " [ 1  3  3 ...  9 13  8]\n",
      " [ 1  3  3 ... 64  7 88]\n",
      " [ 1  3  3 ... 52 33 94]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8194c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accussed committed theft of gold ornaments', 'The victim was a school-going minor girl She was kidnapped by the accused', 'Exchange of words between accused and deceased followed by infliction of single injury leading to death']\n",
      "[[1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 31, 104, 6, 66, 355], [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 34, 10, 11, 257, 2, 140, 154, 38, 10, 155, 14, 4, 12], [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 95, 12, 5, 21, 251, 14, 6, 181, 7, 28]]\n",
      "[4 1 0]\n",
      "[[8.2421058e-04 5.3947456e-02 3.9511539e-02 5.7936933e-02 8.4777993e-01]\n",
      " [9.0276241e-02 5.5591148e-01 1.5170853e-01 1.7205384e-01 3.0049857e-02]\n",
      " [9.9612218e-01 3.1475842e-03 1.3302168e-04 5.4902956e-04 4.8228747e-05]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([390, 364, 302])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  code to test one sentece \n",
    "inp = ['accussed committed theft of gold ornaments','The victim was a school-going minor girl She was kidnapped by the accused','Exchange of words between accused and deceased followed by infliction of single injury leading to death']\n",
    "#accussed raped a girl for 7 days\n",
    "#accussed murdered a man\n",
    "#accussed committed theft of gold ornaments\n",
    "print(inp)\n",
    "inp = tk.texts_to_sequences(inp)\n",
    "inp = sequence.pad_sequences(inp, maxlen=150)\n",
    "index_from=3\n",
    "start_char = 1\n",
    "if start_char is not None:\n",
    "        inp = [[start_char] + [w + index_from for w in x1] for x1 in inp]\n",
    "elif index_from:\n",
    "        inp = [[w + index_from for w in x1] for x1 in inp]\n",
    "num_words = None\n",
    "if not num_words:\n",
    "        num_words = max([max(x1) for x1 in inp])\n",
    "        \n",
    "oov_char = 2\n",
    "skip_top = 0\n",
    "# by convention, use 2 as OOV word\n",
    "# reserve 'index_from' (=3 by default) characters:\n",
    "# 0 (padding), 1 (start), 2 (OOV)\n",
    "if oov_char is not None:\n",
    "        inp = [[w if (skip_top <= w < num_words) else oov_char for w in x1] for x1 in inp]\n",
    "else:\n",
    "        inp = [[w for w in x1 if (skip_top <= w < num_words)] for x1 in inp]\n",
    "print(inp)\n",
    "inp=sequence.pad_sequences(inp, maxlen=151)\n",
    "print(model.predict(inp).argmax(axis=1))\n",
    "print(model.predict(inp))\n",
    "labelencoder_Y.classes_[model.predict(inp).argmax(axis=1)]\n",
    "#improvements \n",
    "#learn on cases outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6183680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73d2a50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himanshu/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "with open('clf.pickle', 'wb') as f:\n",
    "     pickle.dump(model, f)\n",
    "with open('nlpclf.pickle', 'wb') as f:\n",
    "     pickle.dump(tk, f)\n",
    "with open('labelencoder.pickle', 'wb') as f:\n",
    "     pickle.dump(labelencoder_Y, f)\n",
    "os.listdir()\n",
    "['clf.pickle']\n",
    "with open('clf.pickle', 'rb') as f:\n",
    "    modelrr = pickle.load(f)\n",
    "with open('nlpclf.pickle', 'rb') as f:\n",
    "    tokenz = pickle.load(f)\n",
    "with open('labelencoder.pickle', 'rb') as f:\n",
    "    labelencoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "371a1d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accussed committed theft of gold ornaments', 'The victim, Anuva Mondal, was a school-going minor girl. She was kidnapped by the accused, Sk. Manu', 'Exchange of words between accused and deceased followed by infliction of single injury leading to death']\n",
      "[[1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 31, 104, 6, 66, 355], [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 34, 10, 11, 257, 2, 140, 154, 38, 10, 155, 14, 4, 12], [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 95, 12, 5, 21, 251, 14, 6, 181, 7, 28]]\n",
      "[4 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([390, 364, 302])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = ['accussed committed theft of gold ornaments','The victim, Anuva Mondal, was a school-going minor girl. She was kidnapped by the accused, Sk. Manu','Exchange of words between accused and deceased followed by infliction of single injury leading to death']\n",
    "#accussed raped a girl for 7 days\n",
    "#accussed murdered a man\n",
    "#accussed committed theft of gold ornaments\n",
    "print(inp)\n",
    "inp = tokenz.texts_to_sequences(inp)\n",
    "inp = sequence.pad_sequences(inp, maxlen=150)\n",
    "index_from=3\n",
    "start_char = 1\n",
    "if start_char is not None:\n",
    "        inp = [[start_char] + [w + index_from for w in x1] for x1 in inp]\n",
    "elif index_from:\n",
    "        inp = [[w + index_from for w in x1] for x1 in inp]\n",
    "num_words = None\n",
    "if not num_words:\n",
    "        num_words = max([max(x1) for x1 in inp])\n",
    "        \n",
    "oov_char = 2\n",
    "skip_top = 0\n",
    "# by convention, use 2 as OOV word\n",
    "# reserve 'index_from' (=3 by default) characters:\n",
    "# 0 (padding), 1 (start), 2 (OOV)\n",
    "if oov_char is not None:\n",
    "        inp = [[w if (skip_top <= w < num_words) else oov_char for w in x1] for x1 in inp]\n",
    "else:\n",
    "        inp = [[w for w in x1 if (skip_top <= w < num_words)] for x1 in inp]\n",
    "print(inp)\n",
    "inp=sequence.pad_sequences(inp, maxlen=151)\n",
    "print(modelrr.predict(inp).argmax(axis=1))\n",
    "labelencoder.classes_[modelrr.predict(inp).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c490656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.sequential.Sequential"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(modelrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d537eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
