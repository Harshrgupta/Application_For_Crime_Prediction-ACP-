{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "173c94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c5c19519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      on 13th and or 14th day of January 1958, they ...\n",
      "1      According to prosecution, accused 1 and 2 comm...\n",
      "2      The briefly stated prosecution story is that o...\n",
      "3      Sole accused in this case was sent up for tria...\n",
      "4      The case of prosecution as unfolded by report ...\n",
      "                             ...                        \n",
      "172    instant case pertains to the abduction of PWs ...\n",
      "173    Victim, aged about six years, was allegedly ki...\n",
      "174    Planned kidnapping of 3 minor children aged ab...\n",
      "175               Kidnapping in order to compel marriage\n",
      "176       Kidnapping of two children aged 6 and 8 years \n",
      "Name: sentence, Length: 177, dtype: object\n",
      "(array([302, 364, 375, 383, 390]), array([50, 22, 31, 24, 50]))\n"
     ]
    }
   ],
   "source": [
    "# Read the input dataset \n",
    "filepath_dire = {'data': './ACPD.csv'}\n",
    "data_list = []\n",
    "for source, filepath in filepath_dire.items():\n",
    " data = pd.read_csv(filepath, names=['sentence', 'label'], sep=',')\n",
    " data['source'] = source  # Add another column filled with the source name\n",
    " data_list.append(data)\n",
    "data = pd.concat(data_list)\n",
    "x= data['sentence']\n",
    "y= data['label']\n",
    "print(x)\n",
    "\n",
    "#there are {5} unique classes for classification\n",
    "print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "839977a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   4   2 155]\n",
      " [  0   0   0 ...  37  33  91]\n",
      " [  0   0   0 ...   1   3 281]\n",
      " ...\n",
      " [  0   0   0 ... 138  22 127]\n",
      " [  0   0   0 ...   5   4 262]\n",
      " [  0   0   0 ...   2 109  62]]\n"
     ]
    }
   ],
   "source": [
    "tk = Tokenizer(num_words= 400, filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True, split=\" \")\n",
    "tk.fit_on_texts(x)\n",
    "x = tk.texts_to_sequences(x)\n",
    "x = sequence.pad_sequences(x, maxlen=150)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b262eb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      390\n",
      "1      390\n",
      "2      390\n",
      "3      390\n",
      "4      390\n",
      "      ... \n",
      "172    364\n",
      "173    364\n",
      "174    364\n",
      "175    364\n",
      "176    364\n",
      "Name: label, Length: 177, dtype: int64\n",
      "(array([0, 1, 2, 3, 4]), array([50, 22, 31, 24, 50]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_Y = LabelEncoder()\n",
    "print(y)\n",
    "y = labelencoder_Y.fit_transform(y)\n",
    "print(np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e6fb677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras import utils as np_utils\n",
    "print(y)\n",
    "y = np_utils.to_categorical(y, num_classes= 5)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e8f9911b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   4   2 155]\n",
      " [  0   0   0 ...  37  33  91]\n",
      " [  0   0   0 ...   1   3 281]\n",
      " ...\n",
      " [  0   0   0 ... 138  22 127]\n",
      " [  0   0   0 ...   5   4 262]\n",
      " [  0   0   0 ...   2 109  62]]\n",
      "[[  0   0   0 ...   5   3  86]\n",
      " [  0   0   0 ...   4   1  31]\n",
      " [  0   0   0 ...   5  12  25]\n",
      " ...\n",
      " [  0   0   0 ...   2 156   2]\n",
      " [  0   0   0 ...   8  92  42]\n",
      " [  0   0   0 ...  90 105  41]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(150)\n",
    "indices = np.arange(len(x))\n",
    "np.random.shuffle(indices)\n",
    "print(x)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b937a6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 13, 32, 5, 79, 7, 16, 93, 5, 22, 17, 5, 9, 7, 90]\n"
     ]
    }
   ],
   "source": [
    "index_from=4\n",
    "start_char = 1\n",
    "if start_char is not None:\n",
    "        x = [[start_char] + [w + index_from for w in x1] for x1 in x]\n",
    "elif index_from:\n",
    "        x = [[w + index_from for w in x1] for x1 in x]\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "36e93b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141, 151)\n",
      "(141, 5)\n",
      "(36, 151)\n",
      "(36, 5)\n"
     ]
    }
   ],
   "source": [
    "num_words = None\n",
    "if not num_words:\n",
    "        num_words = max([max(x1) for x1 in x])\n",
    "        \n",
    "oov_char = 2\n",
    "skip_top = 0\n",
    "# by convention, use 2 as OOV word\n",
    "# reserve 'index_from' (=3 by default) characters:\n",
    "# 0 (padding), 1 (start), 2 (OOV)\n",
    "if oov_char is not None:\n",
    "        x = [[w if (skip_top <= w < num_words) else oov_char for w in x1] for x1 in x]\n",
    "else:\n",
    "        x = [[w for w in x1 if (skip_top <= w < num_words)] for x1 in x]\n",
    "# split test and train data\n",
    "test_split = 0.2\n",
    "idx = int(len(x) * (1 - test_split))\n",
    "x_train, y_train = np.array(x[:idx]), np.array(y[:idx])\n",
    "x_test, y_test = np.array(x[idx:]), np.array(y[idx:])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "90e26ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (141, 151)\n",
      "x_test shape: (36, 151)\n"
     ]
    }
   ],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=151)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=151)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0101ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himanshu/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 141 samples, validate on 36 samples\n",
      "Epoch 1/15\n",
      "141/141 [==============================] - 3s 18ms/step - loss: 1.5646 - accuracy: 0.2766 - val_loss: 1.6001 - val_accuracy: 0.2500\n",
      "Epoch 2/15\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 1.5030 - accuracy: 0.3617 - val_loss: 1.6041 - val_accuracy: 0.2500\n",
      "Epoch 3/15\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 1.4744 - accuracy: 0.3759 - val_loss: 1.6009 - val_accuracy: 0.3889\n",
      "Epoch 4/15\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 1.4242 - accuracy: 0.4823 - val_loss: 1.6472 - val_accuracy: 0.1667\n",
      "Epoch 5/15\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 1.3106 - accuracy: 0.5603 - val_loss: 1.4576 - val_accuracy: 0.4167\n",
      "Epoch 6/15\n",
      "141/141 [==============================] - 1s 7ms/step - loss: 1.2169 - accuracy: 0.5603 - val_loss: 1.3492 - val_accuracy: 0.4167\n",
      "Epoch 7/15\n",
      "141/141 [==============================] - 1s 7ms/step - loss: 1.0782 - accuracy: 0.5887 - val_loss: 1.2619 - val_accuracy: 0.4167\n",
      "Epoch 8/15\n",
      "141/141 [==============================] - 1s 7ms/step - loss: 0.9424 - accuracy: 0.6028 - val_loss: 1.2111 - val_accuracy: 0.4444\n",
      "Epoch 9/15\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 0.8678 - accuracy: 0.6312 - val_loss: 1.1785 - val_accuracy: 0.4444\n",
      "Epoch 10/15\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 0.7831 - accuracy: 0.6809 - val_loss: 1.1021 - val_accuracy: 0.5556\n",
      "Epoch 11/15\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 0.6957 - accuracy: 0.7376 - val_loss: 1.0968 - val_accuracy: 0.5833\n",
      "Epoch 12/15\n",
      "141/141 [==============================] - 1s 8ms/step - loss: 0.6181 - accuracy: 0.7801 - val_loss: 1.0603 - val_accuracy: 0.6389\n",
      "Epoch 13/15\n",
      "141/141 [==============================] - 1s 7ms/step - loss: 0.5560 - accuracy: 0.7872 - val_loss: 1.0955 - val_accuracy: 0.6944\n",
      "Epoch 14/15\n",
      "141/141 [==============================] - 1s 7ms/step - loss: 0.4866 - accuracy: 0.8511 - val_loss: 0.9568 - val_accuracy: 0.6389\n",
      "Epoch 15/15\n",
      "141/141 [==============================] - 1s 7ms/step - loss: 0.4291 - accuracy: 0.8652 - val_loss: 1.0185 - val_accuracy: 0.6944\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 151, 50)           50000     \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 151, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 149, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_13 (Glo (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 5)                 1255      \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 151,755\n",
      "Trainable params: 151,755\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = 1000\n",
    "maxlen = 151\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "\n",
    "\n",
    "# CNN with max pooling imeplementation \n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=10,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "af78820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  4 ...  9  7 90]\n",
      " [ 1  4  4 ...  8  5 35]\n",
      " [ 1  4  4 ...  9 16 29]\n",
      " ...\n",
      " [ 1  4  4 ... 10 14  9]\n",
      " [ 1  4  4 ... 65  8 89]\n",
      " [ 1  4  4 ... 53 34 95]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8194c7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accussed committed theft of gold ornaments', 'minor girl is raped by accused', 'Exchange of words between accused and deceased followed by infliction of single injury leading to death']\n",
      "[[1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 32, 105, 7, 67, 2], [1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 141, 155, 37, 186, 15, 13], [1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 96, 13, 6, 22, 252, 15, 7, 182, 8, 29]]\n",
      "[4 2 0]\n",
      "[[1.07871365e-05 1.80706452e-03 1.25776455e-02 1.36345923e-02\n",
      "  9.71969843e-01]\n",
      " [8.53230134e-02 3.60035419e-01 3.69897723e-01 1.62827924e-01\n",
      "  2.19158716e-02]\n",
      " [9.96850669e-01 2.51294114e-03 4.33024892e-04 1.99218513e-04\n",
      "  4.19967137e-06]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([390, 375, 302])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  code to test one sentece \n",
    "inp = ['accussed committed theft of gold ornaments','minor girl is raped by accused','Exchange of words between accused and deceased followed by infliction of single injury leading to death']\n",
    "#accussed committed theft of gold ornaments\n",
    "#minor girl is raped by accused\n",
    "#Exchange of words between accused and deceased followed by infliction of single injury leading to death\n",
    "print(inp)\n",
    "inp = tk.texts_to_sequences(inp)\n",
    "inp = sequence.pad_sequences(inp, maxlen=150)\n",
    "index_from=4\n",
    "start_char = 1\n",
    "if start_char is not None:\n",
    "        inp = [[start_char] + [w + index_from for w in x1] for x1 in inp]\n",
    "elif index_from:\n",
    "        inp = [[w + index_from for w in x1] for x1 in inp]\n",
    "num_words = None\n",
    "if not num_words:\n",
    "        num_words = max([max(x1) for x1 in inp])\n",
    "        \n",
    "oov_char = 2\n",
    "skip_top = 0\n",
    "# by convention, use 2 as OOV word\n",
    "# reserve 'index_from' (=3 by default) characters:\n",
    "# 0 (padding), 1 (start), 2 (OOV)\n",
    "if oov_char is not None:\n",
    "        inp = [[w if (skip_top <= w < num_words) else oov_char for w in x1] for x1 in inp]\n",
    "else:\n",
    "        inp = [[w for w in x1 if (skip_top <= w < num_words)] for x1 in inp]\n",
    "print(inp)\n",
    "inp=sequence.pad_sequences(inp, maxlen=151)\n",
    "print(model.predict(inp).argmax(axis=1))\n",
    "print(model.predict(inp))\n",
    "labelencoder_Y.classes_[model.predict(inp).argmax(axis=1)]\n",
    "#improvements \n",
    "#learn on cases outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6183680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "73d2a50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himanshu/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "with open('clf.pickle', 'wb') as f:\n",
    "     pickle.dump(model, f)\n",
    "with open('nlpclf.pickle', 'wb') as f:\n",
    "     pickle.dump(tk, f)\n",
    "with open('labelencoder.pickle', 'wb') as f:\n",
    "     pickle.dump(labelencoder_Y, f)\n",
    "os.listdir()\n",
    "['clf.pickle']\n",
    "with open('clf.pickle', 'rb') as f:\n",
    "    modelrr = pickle.load(f)\n",
    "with open('nlpclf.pickle', 'rb') as f:\n",
    "    tokenz = pickle.load(f)\n",
    "with open('labelencoder.pickle', 'rb') as f:\n",
    "    labelencoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "371a1d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accussed committed theft of gold ornaments', 'This witness has stated that the detenu and his associates extort money at the point of weapons from businessmen, shopkeepers and hawkers of the area; that whoever refuses to give them hapta money are threatened by them and that out of fear nobody complains against them', 'Exchange of words between accused and deceased followed by infliction of single injury leading to death', 'it is established that offender after kidnapping a person keeps him in detention or threatens to cause death or hurt in order to extort ransom, and communicates that demand']\n",
      "[[1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 32, 105, 7, 67, 356], [1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 148, 347, 220, 283, 18, 5, 6, 16, 193, 348, 84, 21, 5, 7, 230, 19, 6, 7, 5, 234, 18, 8, 46, 84, 280, 78, 15, 46, 6, 18, 129, 7, 272, 46], [1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 7, 96, 13, 6, 22, 252, 15, 7, 182, 8, 29], [1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 41, 37, 18, 81, 124, 12, 217, 27, 9, 187, 8, 181, 29, 187, 2, 9, 8, 348, 157, 6, 18, 163]]\n",
      "[4 3 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([390, 383, 302, 302])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = ['accussed committed theft of gold ornaments',\n",
    "       'This witness has stated that the detenu and his associates extort money at the point of weapons from businessmen, shopkeepers and hawkers of the area; that whoever refuses to give them hapta money are threatened by them and that out of fear nobody complains against them',\n",
    "       'Exchange of words between accused and deceased followed by infliction of single injury leading to death',\n",
    "       'it is established that offender after kidnapping a person keeps him in detention or threatens to cause death or hurt in order to extort ransom, and communicates that demand'\n",
    "      ]\n",
    "#accussed raped a girl for 7 days\n",
    "#accussed murdered a man\n",
    "#accussed committed theft of gold ornaments\n",
    "print(inp)\n",
    "inp = tokenz.texts_to_sequences(inp)\n",
    "inp = sequence.pad_sequences(inp, maxlen=150)\n",
    "index_from=4\n",
    "start_char = 1\n",
    "if start_char is not None:\n",
    "        inp = [[start_char] + [w + index_from for w in x1] for x1 in inp]\n",
    "elif index_from:\n",
    "        inp = [[w + index_from for w in x1] for x1 in inp]\n",
    "num_words = None\n",
    "if not num_words:\n",
    "        num_words = max([max(x1) for x1 in inp])\n",
    "        \n",
    "oov_char = 2\n",
    "skip_top = 0\n",
    "# by convention, use 2 as OOV word\n",
    "# reserve 'index_from' (=3 by default) characters:\n",
    "# 0 (padding), 1 (start), 2 (OOV)\n",
    "if oov_char is not None:\n",
    "        inp = [[w if (skip_top <= w < num_words) else oov_char for w in x1] for x1 in inp]\n",
    "else:\n",
    "        inp = [[w for w in x1 if (skip_top <= w < num_words)] for x1 in inp]\n",
    "print(inp)\n",
    "inp=sequence.pad_sequences(inp, maxlen=151)\n",
    "print(modelrr.predict(inp).argmax(axis=1))\n",
    "labelencoder.classes_[modelrr.predict(inp).argmax(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c490656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.sequential.Sequential"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(modelrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d537eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
